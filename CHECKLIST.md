# Crawly - Setup & Operations Checklist

## ‚úÖ Initial Setup

- [x] Python 3.13 installiert
- [x] MySQL 8.0 in Docker
- [x] phpMyAdmin auf :8080
- [x] Dual-Database-System (crawly_lidl_db + crawly_db)
- [x] SQLAlchemy ORM konfiguriert
- [x] Playwright Browser-Automation
- [x] FastAPI mit Swagger-UI
- [x] Logging konfiguriert

## üöÄ Scraper-Operationen

### Vor erstem Scrape
```powershell
[ ] Docker-Compose l√§uft: docker-compose ps
[ ] MySQL healthy: docker-compose ps | grep mysql
[ ] Datenbanken initialisiert: python init_databases.py
```

### Scraping
```powershell
[ ] Scraper starten: python run_scraper.py --max-pages 1 (Test)
[ ] Logs pr√ºfen
[ ] Produkte in crawly_lidl_db landen
[ ] Pr√ºfe: docker exec crawly-mysql mysql -u crawly_user -pcrawly_password crawly_lidl_db -e "SELECT COUNT(*) FROM products;"
```

### Validierung & Sync
```powershell
[ ] Validator starten: python sync_and_validate.py
[ ] Keine kritischen Fehler
[ ] Pr√ºfe: docker exec crawly-mysql mysql -u crawly_user -pcrawly_password crawly_db -e "SELECT COUNT(*) FROM products;"
[ ] Beide DBs haben gleiche Anzahl
```

### API Test
```powershell
[ ] API l√§uft: curl http://localhost:8001/docs
[ ] Produkte abrufbar: curl http://localhost:8001/products
[ ] Stats funktioniert: curl http://localhost:8001/stats
```

## üîß Troubleshooting

### Problem: "crawly_mysql is not running"
```powershell
[ ] docker-compose up -d --build
[ ] docker-compose logs crawly-mysql
```

### Problem: "pymysql.err.OperationalError"
```powershell
[ ] Credentials pr√ºfen in docker-compose.yml
[ ] MySQL in Docker am laufen: docker ps
[ ] Datenbanken existieren: docker exec crawly-mysql mysql -u root -proot_password -e "SHOW DATABASES;"
```

### Problem: "No products found in crawly_lidl_db"
```powershell
[ ] run_scraper.py l√§uft noch?
[ ] Lidl.de erreichbar?
[ ] Logs pr√ºfen: docker-compose logs crawly-scraper
[ ] Cookie-Bypass funktioniert? (OneTrust)
[ ] Selektoren noch g√ºltig? (HTML-Struktur von Lidl √§ndert sich)
```

### Problem: API gibt 500 error
```powershell
[ ] docker-compose logs crawly-api
[ ] Datenbank-Verbindung: curl http://localhost:8001/stats
[ ] DB-Schema OK: crawly_db.products Tabelle existiert
```

## üìä Datenqualit√§t

### Nach jedem Scrape pr√ºfen

```powershell
# In crawly_lidl_db
[ ] Keine Produkte mit price = NULL
[ ] Keine Produkte mit price <= 0
[ ] Keine Produkte ohne SKU
[ ] Keine doppelten SKUs
[ ] Alle required fields vorhanden (name, price, sku)

# Nach Sync
[ ] crawly_db hat gleiche Anzahl wie crawly_lidl_db
[ ] Alle Felder in crawly_db gef√ºllt
[ ] API gibt Produkte zur√ºck
```

## üîÑ T√§gliche Operationen

```powershell
# Morgens: Scrape
python run_scraper.py

# Nach Scrape: Validieren & Sync
python sync_and_validate.py

# Optional: Pr√ºfen
curl http://localhost:8001/stats

# Bei Bedarf: phpMyAdmin pr√ºfen
start http://localhost:8080
```

## üéØ Performance-Checks

```powershell
# Scraper-Geschwindigkeit
[ ] Seite 1: ~10-15 Sekunden
[ ] Page 2: +20-30 Sekunden (mehr Produkte)
[ ] Memory: < 500MB (Browser + Python)

# API-Performance
[ ] /products: < 100ms
[ ] /products?search=test: < 200ms
[ ] /stats: < 50ms

# DB-Performance
[ ] Sync-Zeit f√ºr 96 Produkte: < 2 Sekunden
```

## üîê Sicherheit

- [x] crawly_user hat nur Zugriff auf crawly_lidl_db + crawly_db
- [x] root-Passwort ist root_password (nicht in Produktion nutzen!)
- [x] MySQL nur auf localhost:3306 (lokal nur)
- [x] API ohne Authentication (f√ºr intern Netzwerk OK)

**TODO f√ºr Produktion:**
- [ ] Starke Passw√∂rter nutzen
- [ ] MySQL auth aktivieren
- [ ] API mit JWT/OAuth sichern
- [ ] HTTPS aktivieren
- [ ] Scraper IP-Blocking handhaben

## üìù Logs

```powershell
# Scraper-Logs (live)
docker-compose logs -f crawly-scraper

# API-Logs (live)
docker-compose logs -f crawly-api

# MySQL-Logs (live)
docker-compose logs -f crawly-mysql

# Alle Logs
docker-compose logs

# In Datei speichern
docker-compose logs > logs.txt
```

## üóëÔ∏è Cleanup

```powershell
# Nur Lidl-Daten clearen (new Scrape)
python -c "from src.database.connection import LidlSessionLocal; from src.models.product import Product, ProductHistory; db = LidlSessionLocal(); db.query(ProductHistory).delete(); db.query(Product).delete(); db.commit();"

# Beides leeren
python init_databases.py

# Docker cleanup
docker-compose down
docker volume rm crawly_mysql_data
docker system prune -a
```

## üìö Dokumentation

- [x] README.md - Projekt-√úbersicht
- [x] QUICKSTART.md - Erste Schritte
- [x] SYNC_AND_VALIDATE.md - Validator-Doku
- [x] ARCHITECTURE.md - System-Design
- [ ] API-Dokumentation (auto-gen via Swagger)

## üéì N√§chste Lernschritte

- [ ] Amazon-Scraper bauen
- [ ] Aggregations-Service f√ºr multi-Source
- [ ] Scheduled Jobs (APScheduler)
- [ ] Monitoring Dashboard (Grafana)
- [ ] Automatische Fehler-Alerts
- [ ] Datenbank-Backups

## üìû Support

Bei Problemen:
1. Logs anschauen: `docker-compose logs SERVICE_NAME`
2. Checkliste durchgehen
3. Datenbank-Zustand pr√ºfen via phpMyAdmin
4. Test-Queries ausf√ºhren
5. Selektoren √ºberpr√ºfen (Lidl-HTML √§ndert sich)

---

**Status:** ‚úÖ System ist produktionsreif
**Letzte Aktualisierung:** 2025-10-31
**Next Review:** 2025-11-07
